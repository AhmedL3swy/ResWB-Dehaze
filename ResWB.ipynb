{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXvdv1KWWkIW"
      },
      "outputs": [],
      "source": [
        "#!git clone \"https://github.com/liuh127/NTIRE-2021-Dehazing-DWGAN.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcZY2ZocgGsY"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8dsyUVdWuTJ",
        "outputId": "b5e6cab1-6bf5-4d93-a8dd-b6a33b9ea904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH4eeCUCWqCg"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-41qUzAVWxXN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg16, VGG16_Weights\n",
        "from torchvision.utils import save_image as imwrite\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms.functional as TF\n",
        "import random\n",
        "from math import exp, log10\n",
        "import numbers\n",
        "import os\n",
        "\n",
        "try:\n",
        "  import einops\n",
        "  from einops import rearrange\n",
        "except:\n",
        "  !pip install einops\n",
        "  import einops\n",
        "  from einops import rearrange\n",
        "\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REViZkeNW4g_"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brFXQNT1W6bd"
      },
      "outputs": [],
      "source": [
        "#data augmentation for image rotate\n",
        "def custom_augment(hazy, clean, clean_gray):\n",
        "    augmentation_method = random.choice([0, 1, 2, 3, 4, 5])\n",
        "    rotate_degree = random.choice([90, 180, 270])\n",
        "    '''Rotate'''\n",
        "    if augmentation_method == 0:\n",
        "        hazy = transforms.functional.rotate(hazy, rotate_degree)\n",
        "        clean = transforms.functional.rotate(clean, rotate_degree)\n",
        "        clean_gray = transforms.functional.rotate(clean_gray, rotate_degree)\n",
        "        return hazy, clean, clean_gray\n",
        "    '''Vertical'''\n",
        "    if augmentation_method == 1:\n",
        "        vertical_flip = transforms.RandomVerticalFlip(p=1)\n",
        "        hazy = vertical_flip(hazy)\n",
        "        clean = vertical_flip(clean)\n",
        "        clean_gray = vertical_flip(clean_gray)\n",
        "        return hazy, clean, clean_gray\n",
        "    '''Horizontal'''\n",
        "    if augmentation_method == 2:\n",
        "        horizontal_flip = transforms.RandomHorizontalFlip(p=1)\n",
        "        hazy = horizontal_flip(hazy)\n",
        "        clean = horizontal_flip(clean)\n",
        "        clean_gray = horizontal_flip(clean_gray)\n",
        "        return hazy, clean, clean_gray\n",
        "    '''no change'''\n",
        "    if augmentation_method == 3 or augmentation_method == 4 or augmentation_method == 5:\n",
        "        return hazy, clean, clean_gray\n",
        "\n",
        "class custom_dehaze_train_dataset(Dataset):\n",
        "    def __init__(self, HAZY_path = None, GT_path = None, Image_Size = (256,256), is_train = True):\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "        self.HAZY_path = Path(HAZY_path)\n",
        "        self.GT_path = Path(GT_path)\n",
        "        self.HAZY_Image = sorted(self.HAZY_path.glob(\"*.png\")) # list all the files present in HAZY images folder...\n",
        "        self.GT_Image = sorted(self.GT_path.glob(\"*.png\")) # list all the files present in GT images folder...\n",
        "        self.Image_Size = Image_Size\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        hazy = Image.open(self.HAZY_Image[index])\n",
        "        clean = Image.open(self.GT_Image[index])\n",
        "        if self.is_train:\n",
        "            # clean_gray = clean.convert('L')\n",
        "            clean_gray = clean\n",
        "            #crop a patch\n",
        "            i,j,h,w = transforms.RandomCrop.get_params(hazy, output_size = self.Image_Size)\n",
        "            hazy_ = TF.crop(hazy, i, j, h, w)\n",
        "            clean_ = TF.crop(clean, i, j, h, w)\n",
        "            clean_gray_ = TF.crop(clean_gray, i, j, h, w)\n",
        "\n",
        "            #data argumentation\n",
        "            hazy_arg, clean_arg, clean_gray_arg = custom_augment(hazy_, clean_, clean_gray_)\n",
        "            hazy = self.transform(hazy_arg)\n",
        "            clean = self.transform(clean_arg)\n",
        "            rgb_edged_cv2_x = cv2.Sobel(np.float32(clean_gray_arg), cv2.CV_64F, 1, 0, ksize=3)\n",
        "            rgb_edged_cv2_y = cv2.Sobel(np.float32(clean_gray_arg), cv2.CV_64F, 0, 1, ksize=3)\n",
        "            rgb_edged_cv2 = np.sqrt(np.square(rgb_edged_cv2_x), np.square(rgb_edged_cv2_y))\n",
        "            clean_gray = self.transform(rgb_edged_cv2)\n",
        "            return hazy,clean,clean_gray/255\n",
        "        else:\n",
        "          hazy = self.transform(hazy)\n",
        "          clean = self.transform(clean)\n",
        "          return hazy,clean\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.HAZY_Image) # return length of dataset\n",
        "\n",
        "class dehaze_test_dataset(Dataset):\n",
        "    def __init__(self, HAZY_PATH = None, GT_PATH = None):\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "        self.root_hazy = Path(HAZY_PATH)\n",
        "        self.root_GT = Path(GT_PATH)\n",
        "        self.list_test = sorted(self.root_hazy.glob(\"*.png\")) # list all the files present in HAZY images folder...\n",
        "        self.list_GT = sorted(self.root_GT.glob(\"*.png\")) # list all the files present in GT images folder...\n",
        "        self.file_len = len(self.list_test)\n",
        "    def __getitem__(self, index, is_train=True):\n",
        "        hazy = Image.open(self.list_test[index])\n",
        "        hazy = self.transform(hazy)\n",
        "        #----------- Gives cuda out of memory -----------\n",
        "        hazy_up=hazy[:,0:1152,:]\n",
        "        hazy_down=hazy[:,48:1200,:]\n",
        "\n",
        "        # ----------- Doesn't give cuda out of memory but the separating line is visible -----------\n",
        "        # hazy_up=hazy[:,0:768,:]\n",
        "        # hazy_down=hazy[:,432:1200,:]\n",
        "        name=self.list_test[index].stem\n",
        "        if len(self.list_GT) == 0:\n",
        "          return hazy_up,hazy_down,name\n",
        "        else:\n",
        "          clean=Image.open(self.list_GT[index])\n",
        "          clean = self.transform(clean)\n",
        "          return hazy_up, hazy_down, name, clean \n",
        "    def __len__(self):\n",
        "        return self.file_len\n",
        "\n",
        "class CustomDataLoader(Dataset):\n",
        "    def __init__(self, HAZY_path = None, GT_path = None, image_size = None, resize = None):\n",
        "        self.HAZY_path = Path(HAZY_path)\n",
        "        self.GT_path = Path(GT_path)\n",
        "        self.HAZY_Image = sorted(self.HAZY_path.glob(\"*.png\")) # list all the files present in HAZY images folder...\n",
        "        self.GT_Image = sorted(self.GT_path.glob(\"*.png\")) # list all the files present in GT images folder...\n",
        "        assert len(self.HAZY_Image) == len(self.GT_Image)  \n",
        "        self.resize = resize\n",
        "        if(self.resize):\n",
        "            self.data_transforms = transforms.Compose([transforms.Resize(image_size),\n",
        "                                                        transforms.ToTensor()])\n",
        "        else:\n",
        "            self.data_transforms = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    def load_image(self, index: int, image_type = \"HAZY\") -> Image.Image:\n",
        "        \"Opens an image via a path and returns it.\"\n",
        "\n",
        "        if image_type == \"HAZY\":\n",
        "          image_path = self.HAZY_Image[index]\n",
        "        elif image_type == \"GT\":\n",
        "          image_path = self.GT_Image[index]\n",
        "\n",
        "        return Image.open(image_path)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.HAZY_Image) # return length of dataset\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        HAZY = Image.open(self.HAZY_Image[index])\n",
        "        GT = Image.open(self.GT_Image[index]) \n",
        "        return self.data_transforms(HAZY), self.data_transforms(GT), self.HAZY_Image[index].stem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDlBGnuoXTEK"
      },
      "source": [
        "# Utils_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US2BFpdJXtSx"
      },
      "outputs": [],
      "source": [
        "def to_psnr(frame_out, gt):\n",
        "    mse = F.mse_loss(frame_out, gt, reduction='none')\n",
        "    mse_split = torch.split(mse, 1, dim=0)\n",
        "    mse_list = [torch.mean(torch.squeeze(mse_split[ind])).item() for ind in range(len(mse_split))]\n",
        "    intensity_max = 1.0\n",
        "    psnr_list = [10.0 * log10(intensity_max / mse) for mse in mse_list]\n",
        "    return psnr_list\n",
        "\n",
        "def to_ssim_skimage(dehaze, gt):\n",
        "    dehaze_list = torch.split(dehaze, 1, dim=0)\n",
        "    gt_list = torch.split(gt, 1, dim=0)\n",
        "\n",
        "    dehaze_list_np = [dehaze_list[ind].permute(0, 2, 3, 1).data.cpu().numpy().squeeze() for ind in range(len(dehaze_list))]\n",
        "    gt_list_np = [gt_list[ind].permute(0, 2, 3, 1).data.cpu().numpy().squeeze() for ind in range(len(dehaze_list))]\n",
        "    # ssim_list = [ssim(dehaze_list_np[ind],  gt_list_np[ind], data_range=1, multichannel=True) for ind in range(len(dehaze_list))]\n",
        "    ssim_list = [ssim(dehaze_list_np[ind],  gt_list_np[ind], data_range=1, channel_axis=-1) for ind in range(len(dehaze_list))]\n",
        "\n",
        "    return ssim_list\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "\n",
        "def create_window(window_size, channel=1):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
        "    return window\n",
        "\n",
        "def ssim1(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None):\n",
        "    # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).\n",
        "    if val_range is None:\n",
        "        if torch.max(img1) > 128:\n",
        "            max_val = 255\n",
        "        else:\n",
        "            max_val = 1\n",
        "\n",
        "        if torch.min(img1) < -0.5:\n",
        "            min_val = -1\n",
        "        else:\n",
        "            min_val = 0\n",
        "        L = max_val - min_val\n",
        "    else:\n",
        "        L = val_range\n",
        "\n",
        "    padd = 0\n",
        "    (_, channel, height, width) = img1.size()\n",
        "    if window is None:\n",
        "        real_size = min(window_size, height, width)\n",
        "        window = create_window(real_size, channel=channel).to(img1.device)\n",
        "\n",
        "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
        "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
        "\n",
        "    C1 = (0.01 * L) ** 2\n",
        "    C2 = (0.03 * L) ** 2\n",
        "\n",
        "    v1 = 2.0 * sigma12 + C2\n",
        "    v2 = sigma1_sq + sigma2_sq + C2\n",
        "    cs = v1 / v2  # contrast sensitivity\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
        "\n",
        "    if size_average:\n",
        "        cs = cs.mean()\n",
        "        ret = ssim_map.mean()\n",
        "    else:\n",
        "        cs = cs.mean(1).mean(1).mean(1)\n",
        "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "    if full:\n",
        "        return ret, cs\n",
        "    return ret\n",
        "\n",
        "\n",
        "def msssim(img1, img2, window_size=11, size_average=True, val_range=None, normalize=False):\n",
        "    device = img1.device\n",
        "    weights = torch.FloatTensor([0.0448, 0.2856, 0.3001, 0.2363, 0.1333]).to(device)\n",
        "    levels = weights.size()[0]\n",
        "    mssim = []\n",
        "    mcs = []\n",
        "    for _ in range(levels):\n",
        "        sim, cs = ssim1(img1, img2, window_size=window_size, size_average=size_average, full=True, val_range=val_range)\n",
        "        mssim.append(sim)\n",
        "        mcs.append(cs)\n",
        "\n",
        "        img1 = F.avg_pool2d(img1, (2, 2))\n",
        "        img2 = F.avg_pool2d(img2, (2, 2))\n",
        "\n",
        "    \n",
        "    mssim = torch.stack(mssim)\n",
        "    mcs = torch.stack(mcs)\n",
        "\n",
        "    # Normalize (to avoid NaNs during training unstable models, not compliant with original definition)\n",
        "    if normalize:\n",
        "        mssim = (mssim + 1) / 2\n",
        "        mcs = (mcs + 1) / 2\n",
        "\n",
        "    pow1 = mcs ** weights\n",
        "    pow2 = mssim ** weights\n",
        "    # From Matlab implementation https://ece.uwaterloo.ca/~z70wang/research/iwssim/\n",
        "    output = torch.prod(pow1[:-1] * pow2[-1])\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbNGCutUX0tF"
      },
      "source": [
        "# Loss Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWTfLJGHX2J4"
      },
      "outputs": [],
      "source": [
        "# --- Perceptual loss network  --- #\n",
        "class LossNetwork(torch.nn.Module):\n",
        "    def __init__(self, vgg_model):\n",
        "        super(LossNetwork, self).__init__()\n",
        "        self.vgg_layers = vgg_model\n",
        "        self.layer_name_mapping = {\n",
        "            '3': \"relu1_2\",\n",
        "            '8': \"relu2_2\",\n",
        "            '15': \"relu3_3\"\n",
        "        }\n",
        "\n",
        "    def output_features(self, x):\n",
        "        output = {}\n",
        "        for name, module in self.vgg_layers._modules.items():\n",
        "            x = module(x)\n",
        "            if name in self.layer_name_mapping:\n",
        "                output[self.layer_name_mapping[name]] = x\n",
        "        return list(output.values())\n",
        "\n",
        "    def forward(self, dehaze, gt):\n",
        "        loss = []\n",
        "        dehaze_features = self.output_features(dehaze)\n",
        "        gt_features = self.output_features(gt)\n",
        "        for dehaze_feature, gt_feature in zip(dehaze_features, gt_features):\n",
        "            loss.append(F.mse_loss(dehaze_feature, gt_feature))\n",
        "        return sum(loss)/len(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIotfL5nX3LH"
      },
      "source": [
        "# Restormer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rzj97UKqX7GX"
      },
      "outputs": [],
      "source": [
        "## Restormer: Efficient Transformer for High-Resolution Image Restoration\n",
        "## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang\n",
        "## https://arxiv.org/abs/2111.09881\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Layer Norm\n",
        "\n",
        "def to_3d(x):\n",
        "    return rearrange(x, 'b c h w -> b (h w) c')\n",
        "    # flatten = nn.Flatten(2,3)\n",
        "    # return flatten(x).permute(0,2,1)\n",
        "\n",
        "def to_4d(x,h,w):\n",
        "    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)\n",
        "    # unflatten = nn.Unflatten(1,(h,w))\n",
        "    # return unflatten(x).permute(0,3,1,2)\n",
        "\n",
        "class BiasFree_LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape):\n",
        "        super(BiasFree_LayerNorm, self).__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            normalized_shape = (normalized_shape,)\n",
        "        normalized_shape = torch.Size(normalized_shape)\n",
        "\n",
        "        assert len(normalized_shape) == 1\n",
        "\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
        "        return x / torch.sqrt(sigma+1e-5) * self.weight\n",
        "\n",
        "class WithBias_LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape):\n",
        "        super(WithBias_LayerNorm, self).__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            normalized_shape = (normalized_shape,)\n",
        "        normalized_shape = torch.Size(normalized_shape)\n",
        "\n",
        "        assert len(normalized_shape) == 1\n",
        "\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu = x.mean(-1, keepdim=True)\n",
        "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
        "        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim, LayerNorm_type):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        if LayerNorm_type =='BiasFree':\n",
        "            self.body = BiasFree_LayerNorm(dim)\n",
        "        else:\n",
        "            self.body = WithBias_LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, w = x.shape[-2:]\n",
        "        return to_4d(self.body(to_3d(x)), h, w)\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Gated-Dconv Feed-Forward Network (GDFN)\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        hidden_features = int(dim*ffn_expansion_factor)\n",
        "\n",
        "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
        "\n",
        "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
        "\n",
        "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project_in(x)\n",
        "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
        "        x = F.gelu(x1) * x2\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Multi-DConv Head Transposed Self-Attention (MDTA)\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, bias):\n",
        "        super(Attention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
        "\n",
        "        self.qkv = nn.Conv2d(dim, dim*3, kernel_size=1, bias=bias)\n",
        "        self.qkv_dwconv = nn.Conv2d(dim*3, dim*3, kernel_size=3, stride=1, padding=1, groups=dim*3, bias=bias)\n",
        "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "\n",
        "        qkv = self.qkv_dwconv(self.qkv(x))\n",
        "        q,k,v = qkv.chunk(3, dim=1)   \n",
        "        \n",
        "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "\n",
        "        q = torch.nn.functional.normalize(q, dim=-1)\n",
        "        k = torch.nn.functional.normalize(k, dim=-1)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        out = (attn @ v)\n",
        "        \n",
        "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
        "\n",
        "        out = self.project_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.norm1 = LayerNorm(dim, LayerNorm_type)\n",
        "        self.attn = Attention(dim, num_heads, bias)\n",
        "        self.norm2 = LayerNorm(dim, LayerNorm_type)\n",
        "        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.ffn(self.norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Overlapped image patch embedding with 3x3 Conv\n",
        "class OverlapPatchEmbed(nn.Module):\n",
        "    def __init__(self, in_c=3, embed_dim=48, bias=False):\n",
        "        super(OverlapPatchEmbed, self).__init__()\n",
        "\n",
        "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Resizing modules\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, n_feat):\n",
        "        super(Downsample, self).__init__()\n",
        "\n",
        "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat//2, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                  nn.PixelUnshuffle(2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, n_feat):\n",
        "        super(Upsample, self).__init__()\n",
        "\n",
        "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                  nn.PixelShuffle(2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "##########################################################################\n",
        "##---------- Restormer -----------------------\n",
        "class Restormer(nn.Module):\n",
        "    def __init__(self, \n",
        "        inp_channels=3, \n",
        "        out_channels=3, \n",
        "        dim = 48,\n",
        "        num_blocks = [4,6,6,8], \n",
        "        num_refinement_blocks = 4,\n",
        "        heads = [1,2,4,8],\n",
        "        ffn_expansion_factor = 2.66,\n",
        "        bias = False,\n",
        "        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
        "        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n",
        "    ):\n",
        "\n",
        "        super(Restormer, self).__init__()\n",
        "\n",
        "        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n",
        "\n",
        "        self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "        \n",
        "        self.down1_2 = Downsample(dim) ## From Level 1 to Level 2\n",
        "        self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
        "        \n",
        "        self.down2_3 = Downsample(int(dim*2**1)) ## From Level 2 to Level 3\n",
        "        self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
        "\n",
        "        self.down3_4 = Downsample(int(dim*2**2)) ## From Level 3 to Level 4\n",
        "        self.latent = nn.Sequential(*[TransformerBlock(dim=int(dim*2**3), num_heads=heads[3], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[3])])\n",
        "        \n",
        "        self.up4_3 = Upsample(int(dim*2**3)) ## From Level 4 to Level 3\n",
        "        self.reduce_chan_level3 = nn.Conv2d(int(dim*2**3), int(dim*2**2), kernel_size=1, bias=bias)\n",
        "        self.decoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
        "\n",
        "\n",
        "        self.up3_2 = Upsample(int(dim*2**2)) ## From Level 3 to Level 2\n",
        "        self.reduce_chan_level2 = nn.Conv2d(int(dim*2**2), int(dim*2**1), kernel_size=1, bias=bias)\n",
        "        self.decoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
        "        \n",
        "        self.up2_1 = Upsample(int(dim*2**1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n",
        "\n",
        "        self.decoder_level1 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "        \n",
        "        self.refinement = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_refinement_blocks)])\n",
        "        \n",
        "        #### For Dual-Pixel Defocus Deblurring Task ####\n",
        "        self.dual_pixel_task = dual_pixel_task\n",
        "        if self.dual_pixel_task:\n",
        "            self.skip_conv = nn.Conv2d(dim, int(dim*2**1), kernel_size=1, bias=bias)\n",
        "        ###########################\n",
        "            \n",
        "        self.output = nn.Conv2d(int(dim*2**1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
        "\n",
        "    def forward(self, inp_img):\n",
        "        inp_enc_level1 = self.patch_embed(inp_img)\n",
        "        out_enc_level1 = self.encoder_level1(inp_enc_level1)\n",
        "        inp_enc_level1 = inp_enc_level1.detach().cpu()\n",
        "        del inp_enc_level1\n",
        "        gc.collect()\n",
        "        \n",
        "        inp_enc_level2 = self.down1_2(out_enc_level1)\n",
        "        out_enc_level2 = self.encoder_level2(inp_enc_level2)\n",
        "        inp_enc_level2 = inp_enc_level2.detach().cpu()\n",
        "        del inp_enc_level2\n",
        "        gc.collect()\n",
        "\n",
        "        inp_enc_level3 = self.down2_3(out_enc_level2)\n",
        "        out_enc_level3 = self.encoder_level3(inp_enc_level3) \n",
        "        inp_enc_level3 = inp_enc_level3.detach().cpu()\n",
        "        del inp_enc_level3\n",
        "        gc.collect()\n",
        "\n",
        "        inp_enc_level4 = self.down3_4(out_enc_level3)        \n",
        "        latent = self.latent(inp_enc_level4) \n",
        "        inp_enc_level4 = inp_enc_level4.detach().cpu()\n",
        "        del inp_enc_level4\n",
        "        gc.collect()\n",
        "                        \n",
        "        inp_dec_level3 = self.up4_3(latent)\n",
        "        latent = latent.detach().cpu()\n",
        "        del latent\n",
        "        gc.collect()\n",
        "\n",
        "        inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n",
        "        out_enc_level3 = out_enc_level3.detach().cpu()\n",
        "        del out_enc_level3\n",
        "        gc.collect()\n",
        "\n",
        "        inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n",
        "        out_dec_level3 = self.decoder_level3(inp_dec_level3) \n",
        "        inp_dec_level3 = inp_dec_level3.detach().cpu()\n",
        "        del inp_dec_level3\n",
        "        gc.collect()\n",
        "\n",
        "        inp_dec_level2 = self.up3_2(out_dec_level3)\n",
        "        out_dec_level3 = out_dec_level3.detach().cpu()\n",
        "        del out_dec_level3\n",
        "        gc.collect()\n",
        "\n",
        "        inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n",
        "        out_enc_level2 = out_enc_level2.detach().cpu()\n",
        "        del out_enc_level2\n",
        "        gc.collect()\n",
        "\n",
        "        inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n",
        "        out_dec_level2 = self.decoder_level2(inp_dec_level2) \n",
        "        inp_dec_level2 = inp_dec_level2.detach().cpu()\n",
        "        del inp_dec_level2\n",
        "        gc.collect()\n",
        "\n",
        "        inp_dec_level1 = self.up2_1(out_dec_level2)\n",
        "        out_dec_level2 = out_dec_level2.detach().cpu()\n",
        "        del out_dec_level2\n",
        "        gc.collect()\n",
        "        \n",
        "        inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n",
        "        out_enc_level1 = out_enc_level1.detach().cpu()\n",
        "        del out_enc_level1\n",
        "        gc.collect()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n",
        "        inp_dec_level1 = inp_dec_level1.detach().cpu()\n",
        "        del inp_dec_level1\n",
        "        gc.collect()\n",
        "\n",
        "        out_dec_level1 = self.refinement(out_dec_level1)\n",
        "        out_dec_level1 = self.output(out_dec_level1) + inp_img\n",
        "\n",
        "        return out_dec_level1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4OlBr-mX8OQ"
      },
      "source": [
        "# Haze Density Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSiCDpOyYBJT"
      },
      "outputs": [],
      "source": [
        "def blockUNet1(in_c, out_c, name, transposed=False, bn=False, relu=True, dropout=False):\n",
        "    block = nn.Sequential()\n",
        "    if relu:\n",
        "        block.add_module('%s_relu' % name, nn.ReLU(inplace=True))\n",
        "    else:\n",
        "        block.add_module('%s_leakyrelu' % name, nn.LeakyReLU(0.2, inplace=True))\n",
        "    if not transposed:\n",
        "        block.add_module('%s_conv' % name, nn.Conv2d(in_c, out_c, 4, 2, 1, bias=False))\n",
        "    else:\n",
        "        block.add_module('%s_tconv' % name, nn.ConvTranspose2d(in_c, out_c, 4, 2, 1, bias=False))\n",
        "    if bn:\n",
        "        block.add_module('%s_bn' % name, nn.BatchNorm2d(out_c))\n",
        "    if dropout:\n",
        "        block.add_module('%s_dropout' % name, nn.Dropout2d(0.5, inplace=True))\n",
        "    return block\n",
        "\n",
        "\n",
        "class HazeDensityMap(nn.Module):\n",
        "    \"\"\"\n",
        "    this is a class for generating haze density map taken from Trident paper\n",
        "    for the pre-trained weights it's in the graduation project drive folder\n",
        "        Graduation Project/HM.pt\n",
        "\n",
        "    it has a bit funny usage for the feed forward you can find the functions in the utils file\n",
        "    and an example for the usage in the notebook:\n",
        "        https://colab.research.google.com/drive/1Ngj5rMHFh1BMWUotsgEVJulpwIbgLP6x#scrollTo=Z3Xr6hqAfuXC\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_nc=3, output_nc=3, nf=8):\n",
        "        super(HazeDensityMap, self).__init__()\n",
        "        # input is 256 x 256\n",
        "        layer_idx = 1\n",
        "        name = 'layer%d' % layer_idx\n",
        "        layer1 = nn.Sequential()\n",
        "        layer1.add_module(name, nn.Conv2d(input_nc, nf, 4, 2, 1, bias=False))\n",
        "        # input is 128 x 128\n",
        "        layer_idx += 1\n",
        "        name = 'layer%d' % layer_idx\n",
        "        layer2 = blockUNet1(nf, nf * 2, name, transposed=False, bn=True, relu=False, dropout=False)\n",
        "        # input is 64 x 64\n",
        "        layer_idx += 1\n",
        "        name = 'layer%d' % layer_idx\n",
        "        layer3 = blockUNet1(nf * 2, nf * 4, name, transposed=False, bn=True, relu=False, dropout=False)\n",
        "        # input is 32\n",
        "        layer_idx += 1\n",
        "        name = 'layer%d' % layer_idx\n",
        "        layer4 = blockUNet1(nf * 4, nf * 8, name, transposed=False, bn=True, relu=False, dropout=False)\n",
        "        # input is 16\n",
        "        layer_idx += 1\n",
        "        name = 'layer%d' % layer_idx\n",
        "        layer5 = blockUNet1(nf * 8, nf * 8, name, transposed=False, bn=True, relu=False, dropout=False)\n",
        "        # input is 8\n",
        "        layer_idx += 1\n",
        "        name = 'layer%d' % layer_idx\n",
        "        layer6 = blockUNet1(nf * 8, nf * 8, name, transposed=False, bn=False, relu=False, dropout=False)\n",
        "\n",
        "        ## NOTE: decoder\n",
        "        # input is 4\n",
        "        layer_idx -= 1\n",
        "        name = 'dlayer%d' % layer_idx\n",
        "\n",
        "        # dlayer6 = blockUNet1(nf*16, nf*8, name, transposed=True, bn=True, relu=True, dropout=True)\n",
        "        dlayer6 = blockUNet1(nf * 8, nf * 8, name, transposed=True, bn=True, relu=True, dropout=False)\n",
        "        # input is 8\n",
        "        layer_idx -= 1\n",
        "        name = 'dlayer%d' % layer_idx\n",
        "\n",
        "        dlayer5 = blockUNet1(nf * 16, nf * 8, name, transposed=True, bn=True, relu=True, dropout=False)\n",
        "        # input is 16\n",
        "        layer_idx -= 1\n",
        "        name = 'dlayer%d' % layer_idx\n",
        "\n",
        "        dlayer4 = blockUNet1(nf * 16, nf * 4, name, transposed=True, bn=True, relu=True, dropout=False)\n",
        "        # input is 32\n",
        "        layer_idx -= 1\n",
        "        name = 'dlayer%d' % layer_idx\n",
        "\n",
        "        dlayer3 = blockUNet1(nf * 8, nf * 2, name, transposed=True, bn=True, relu=True, dropout=False)\n",
        "        # input is 64\n",
        "        layer_idx -= 1\n",
        "        name = 'dlayer%d' % layer_idx\n",
        "\n",
        "        dlayer2 = blockUNet1(nf * 4, nf, name, transposed=True, bn=True, relu=True, dropout=False)\n",
        "        # input is 128\n",
        "        layer_idx -= 1\n",
        "        name = 'dlayer%d' % layer_idx\n",
        "        dlayer1 = blockUNet1(nf * 2, nf * 2, name, transposed=True, bn=True, relu=True, dropout=False)\n",
        "\n",
        "        self.layer1 = layer1\n",
        "        self.layer2 = layer2\n",
        "        self.layer3 = layer3\n",
        "        self.layer4 = layer4\n",
        "        self.layer5 = layer5\n",
        "        self.layer6 = layer6\n",
        "        self.dlayer6 = dlayer6\n",
        "        self.dlayer5 = dlayer5\n",
        "        self.dlayer4 = dlayer4\n",
        "        self.dlayer3 = dlayer3\n",
        "        self.dlayer2 = dlayer2\n",
        "        self.dlayer1 = dlayer1\n",
        "        self.tail_conv = nn.Conv2d(nf * 2, output_nc, 3, padding=1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        mod1 = h % 64\n",
        "        mod2 = w % 64\n",
        "        if (mod1):\n",
        "            down1 = 64 - mod1\n",
        "            x = F.pad(x, (0, 0, 0, down1), \"reflect\")\n",
        "        if (mod2):\n",
        "            down2 = 64 - mod2\n",
        "            x = F.pad(x, (0, down2, 0, 0), \"reflect\")\n",
        "\n",
        "        out1 = self.layer1(x)\n",
        "        out2 = self.layer2(out1)\n",
        "        out3 = self.layer3(out2)\n",
        "        out4 = self.layer4(out3)\n",
        "        out5 = self.layer5(out4)\n",
        "        out6 = self.layer6(out5)\n",
        "        dout6 = self.dlayer6(out6)\n",
        "        dout6_out5 = torch.cat([dout6, out5], 1)\n",
        "        dout5 = self.dlayer5(dout6_out5)\n",
        "        dout5_out4 = torch.cat([dout5, out4], 1)\n",
        "        dout4 = self.dlayer4(dout5_out4)\n",
        "        dout4_out3 = torch.cat([dout4, out3], 1)\n",
        "        dout3 = self.dlayer3(dout4_out3)\n",
        "        dout3_out2 = torch.cat([dout3, out2], 1)\n",
        "        dout2 = self.dlayer2(dout3_out2)\n",
        "        dout2_out1 = torch.cat([dout2, out1], 1)\n",
        "        dout1 = self.dlayer1(dout2_out1)\n",
        "        dout1 = self.tail_conv(dout1)\n",
        "\n",
        "        if (mod1): dout1 = dout1[:, :, :-down1, :]\n",
        "        if (mod2): dout1 = dout1[:, :, :, :-down2]\n",
        "\n",
        "        return dout1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKD_y5b4YEIw"
      },
      "source": [
        "# Sobel_UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFVu4IVUYHdC"
      },
      "outputs": [],
      "source": [
        "class Sobel_UNet(nn.Module):\n",
        "    def __init__(self, input_nc=3, output_nc=3, nf=8):\n",
        "        super(Sobel_UNet, self).__init__()\n",
        "        # input is 256 x 256\n",
        "        layer_idx = 1\n",
        "        name = 'layer%d' % layer_idx\n",
        "        layer1 = nn.Sequential()\n",
        "        layer1.add_module(name, nn.Conv2d(input_nc, nf, 4, 2, 1, bias=False))\n",
        "        # input is 128 x 128\n",
        "        layer_idx += 1\n",
        "        name = 'layer%d' % layer_idx\n",
        "        layer2 = blockUNet1(nf, nf * 2, name, transposed=False, bn=True, relu=False, dropout=False)\n",
        "        # input is 64 x 64\n",
        "        layer_idx += 1\n",
        "        name = 'layer%d' % layer_idx\n",
        "        layer3 = blockUNet1(nf * 2, nf * 4, name, transposed=False, bn=True, relu=False, dropout=False)\n",
        "        # input is 32\n",
        "        layer_idx += 1\n",
        "        name = 'layer%d' % layer_idx\n",
        "        layer4 = blockUNet1(nf * 4, nf * 8, name, transposed=False, bn=True, relu=False, dropout=False)\n",
        "        # input is 16\n",
        "        layer_idx += 1\n",
        "        name = 'layer%d' % layer_idx\n",
        "        layer5 = blockUNet1(nf * 8, nf * 8, name, transposed=False, bn=True, relu=False, dropout=False)\n",
        "        # input is 8\n",
        "        layer_idx += 1\n",
        "        name = 'layer%d' % layer_idx\n",
        "        layer6 = blockUNet1(nf * 8, nf * 8, name, transposed=False, bn=False, relu=False, dropout=False)\n",
        "\n",
        "        ## NOTE: decoder\n",
        "        # input is 4\n",
        "        layer_idx -= 1\n",
        "        name = 'dlayer%d' % layer_idx\n",
        "\n",
        "        # dlayer6 = blockUNet1(nf*16, nf*8, name, transposed=True, bn=True, relu=True, dropout=True)\n",
        "        dlayer6 = blockUNet1(nf * 8, nf * 8, name, transposed=True, bn=True, relu=True, dropout=False)\n",
        "        # input is 8\n",
        "        layer_idx -= 1\n",
        "        name = 'dlayer%d' % layer_idx\n",
        "\n",
        "        dlayer5 = blockUNet1(nf * 16, nf * 8, name, transposed=True, bn=True, relu=True, dropout=False)\n",
        "        # input is 16\n",
        "        layer_idx -= 1\n",
        "        name = 'dlayer%d' % layer_idx\n",
        "\n",
        "        dlayer4 = blockUNet1(nf * 16, nf * 4, name, transposed=True, bn=True, relu=True, dropout=False)\n",
        "        # input is 32\n",
        "        layer_idx -= 1\n",
        "        name = 'dlayer%d' % layer_idx\n",
        "\n",
        "        dlayer3 = blockUNet1(nf * 8, nf * 2, name, transposed=True, bn=True, relu=True, dropout=False)\n",
        "        # input is 64\n",
        "        layer_idx -= 1\n",
        "        name = 'dlayer%d' % layer_idx\n",
        "\n",
        "        dlayer2 = blockUNet1(nf * 4, nf, name, transposed=True, bn=True, relu=True, dropout=False)\n",
        "        # input is 128\n",
        "        layer_idx -= 1\n",
        "        name = 'dlayer%d' % layer_idx\n",
        "        dlayer1 = blockUNet1(nf * 2, nf * 2, name, transposed=True, bn=True, relu=True, dropout=False)\n",
        "\n",
        "        self.layer1 = layer1\n",
        "        self.layer2 = layer2\n",
        "        self.layer3 = layer3\n",
        "        self.layer4 = layer4\n",
        "        self.layer5 = layer5\n",
        "        self.layer6 = layer6\n",
        "        self.dlayer6 = dlayer6\n",
        "        self.dlayer5 = dlayer5\n",
        "        self.dlayer4 = dlayer4\n",
        "        self.dlayer3 = dlayer3\n",
        "        self.dlayer2 = dlayer2\n",
        "        self.dlayer1 = dlayer1\n",
        "        self.tail_conv = nn.Conv2d(nf * 2, output_nc, 3, padding=1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape  \n",
        "        mod1 = h % 64\n",
        "        mod2 = w % 64\n",
        "        if (mod1):\n",
        "            down1 = 64 - mod1\n",
        "            x = F.pad(x, (0, 0, 0, down1), \"reflect\")\n",
        "        if (mod2):\n",
        "            down2 = 64 - mod2\n",
        "            x = F.pad(x, (0, down2, 0, 0), \"reflect\")\n",
        "        out1 = self.layer1(x)\n",
        "        out2 = self.layer2(out1)\n",
        "        out3 = self.layer3(out2)\n",
        "        out4 = self.layer4(out3)\n",
        "        out5 = self.layer5(out4)\n",
        "        out6 = self.layer6(out5)\n",
        "        dout6 = self.dlayer6(out6)\n",
        "        dout6_out5 = torch.cat([dout6, out5], 1)\n",
        "        dout5 = self.dlayer5(dout6_out5)\n",
        "        dout5_out4 = torch.cat([dout5, out4], 1)\n",
        "        dout4 = self.dlayer4(dout5_out4)\n",
        "        dout4_out3 = torch.cat([dout4, out3], 1)\n",
        "        dout3 = self.dlayer3(dout4_out3)\n",
        "        dout3_out2 = torch.cat([dout3, out2], 1)\n",
        "        dout2 = self.dlayer2(dout3_out2)\n",
        "        dout2_out1 = torch.cat([dout2, out1], 1)\n",
        "        dout1 = self.dlayer1(dout2_out1)\n",
        "        dout1 = self.tail_conv(dout1)\n",
        "        if (mod1): dout1 = dout1[:, :, :-down1, :]\n",
        "        if (mod2): dout1 = dout1[:, :, :, :-down2]\n",
        "        return dout1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfmycrBrUoaJ"
      },
      "source": [
        "# Adaptive White Balancing (AWB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUgrEAkJQ9sx"
      },
      "source": [
        "## deep_wb_blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsMM5NvBQ_i6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        " Main blocks of the network\n",
        " Copyright (c) 2019 Samsung Electronics Co., Ltd. All Rights Reserved\n",
        " If you use this code, please cite the following paper:\n",
        " Mahmoud Afifi and Michael S Brown. Deep White-Balance Editing. In CVPR, 2020.\n",
        "\"\"\"\n",
        "__author__ = \"Mahmoud Afifi\"\n",
        "__credits__ = [\"Mahmoud Afifi\"]\n",
        "\n",
        "class DoubleConvBlock(nn.Module):\n",
        "    \"\"\"double conv layers block\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    \"\"\"Downscale block: maxpool -> double conv block\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConvBlock(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class BridgeDown(nn.Module):\n",
        "    \"\"\"Downscale bottleneck block: maxpool -> conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class BridgeUP(nn.Module):\n",
        "    \"\"\"Downscale bottleneck block: conv -> transpose conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv_up = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_up(x)\n",
        "\n",
        "\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    \"\"\"Upscale block: double conv block -> transpose conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = DoubleConvBlock(in_channels * 2, in_channels)\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return torch.relu(self.up(x))\n",
        "\n",
        "\n",
        "class OutputBlock(nn.Module):\n",
        "    \"\"\"Output block: double conv block -> output conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.out_conv = nn.Sequential(\n",
        "            DoubleConvBlock(in_channels * 2, in_channels),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1))\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.out_conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvYOlzhBUu-e"
      },
      "source": [
        "## deep_wb_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w309IB_1RI73"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        " Constructs network architecture\n",
        " Copyright (c) 2019 Samsung Electronics Co., Ltd. All Rights Reserved\n",
        " If you use this code, please cite the following paper:\n",
        " Mahmoud Afifi and Michael S Brown. Deep White-Balance Editing. In CVPR, 2020.\n",
        "\"\"\"\n",
        "__author__ = \"Mahmoud Afifi\"\n",
        "__credits__ = [\"Mahmoud Afifi\"]\n",
        "\n",
        "class deepWBNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(deepWBNet, self).__init__()\n",
        "        self.n_channels = 3\n",
        "        self.encoder_inc = DoubleConvBlock(self.n_channels, 24)\n",
        "        self.encoder_down1 = DownBlock(24, 48)\n",
        "        self.encoder_down2 = DownBlock(48, 96)\n",
        "        self.encoder_down3 = DownBlock(96, 192)\n",
        "        self.encoder_bridge_down = BridgeDown(192, 384)\n",
        "        self.awb_decoder_bridge_up = BridgeUP(384, 192)\n",
        "        self.awb_decoder_up1 = UpBlock(192, 96)\n",
        "        self.awb_decoder_up2 = UpBlock(96, 48)\n",
        "        self.awb_decoder_up3 = UpBlock(48, 24)\n",
        "        self.awb_decoder_out = OutputBlock(24, self.n_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.encoder_inc(x)\n",
        "        x2 = self.encoder_down1(x1)\n",
        "        x3 = self.encoder_down2(x2)\n",
        "        x4 = self.encoder_down3(x3)\n",
        "        x5 = self.encoder_bridge_down(x4)\n",
        "        x_awb = self.awb_decoder_bridge_up(x5)\n",
        "        x_awb = self.awb_decoder_up1(x_awb, x4)\n",
        "        x_awb = self.awb_decoder_up2(x_awb, x3)\n",
        "        x_awb = self.awb_decoder_up3(x_awb, x2)\n",
        "        awb = self.awb_decoder_out(x_awb, x1)\n",
        "        return awb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJVyF2-FVL0V"
      },
      "source": [
        "# Custom fusion net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsvBWxRQVU31"
      },
      "outputs": [],
      "source": [
        "class GBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super(GBlock, self).__init__()\n",
        "\n",
        "        self.c = nn.Conv2d(in_c + in_c, out_c, 1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x1, x2, x3):\n",
        "        x = torch.cat([x1, x2], axis = 1)\n",
        "        x = self.c(x)\n",
        "        x = self.sig(x)\n",
        "        x = x * x3\n",
        "        return x\n",
        "class Custom_fusion_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Custom_fusion_net, self).__init__()\n",
        "        self.restormer = Restormer()\n",
        "        checkpoint = torch.load(\"/content/drive/Shareddrives/Untitled shared drive/CANT_Haze/Weights/motion_deblurring.pth\")\n",
        "        self.restormer.load_state_dict(checkpoint['params'])\n",
        "        self.sobel_UNet = Sobel_UNet()\n",
        "        self.haze_density = HazeDensityMap()\n",
        "        self.haze_density.load_state_dict(torch.load(\"/content/drive/MyDrive/Copy of HM.pt\"))\n",
        "        self.GBlock = GBlock(3,3)\n",
        "        self.awb =  deepWBNet()\n",
        "        checkpoints = torch.load(\"/content/drive/Shareddrives/Untitled shared drive/CANT_Haze/Weights/net_awb.pth\")\n",
        "        self.awb.load_state_dict(checkpoints['state_dict'])\n",
        "    def forward(self, input):\n",
        "        restormer=self.restormer(input)\n",
        "        x = self.haze_density(input)\n",
        "        hazy_sobel = self.sobel_UNet(restormer)\n",
        "        x = self.GBlock(x,hazy_sobel,restormer)\n",
        "        x = self.awb(x)    \n",
        "        return x , hazy_sobel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWtBU6qKmPqJ"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZy9dgbBmWUE"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(512, 1024, kernel_size=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(1024, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        return torch.sigmoid(self.net(x).view(batch_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAQoHg9DqvDO"
      },
      "source": [
        "# Testing Without Resizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsDXQCWYnd4U"
      },
      "outputs": [],
      "source": [
        "# # VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/valid_dense/HAZY/\"\n",
        "# # VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/valid_dense/GT/\"\n",
        "\n",
        "# # VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_Hazy/\"\n",
        "# # VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_GT/\"\n",
        "\n",
        "# VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_Hazy/\"\n",
        "# VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_GT/\"\n",
        "\n",
        "# #----------- Gives Cuda out of memory -----------\n",
        "# RESIZE = False \n",
        "\n",
        "# VAL_BATCH_SIZE = 1\n",
        "# NUM_WORKERS = 0\n",
        "\n",
        "\n",
        "# # --- output picture and check point --- #\n",
        "# #G_model_save_dir = \"/content/drive/MyDrive/Graduation Project/CANT_Haze/Weights/Generator_NH_Restormer_Twice_HM_GBlock_AWB_Sobel_Best.pth\"\n",
        "# G_model_save_dir = \"/content/drive/Shareddrives/Untitled shared drive/CANT_Haze/Weights/Generator_NH_Restormer_Twice_HM_GBlock_AWB_Sobel_Best.pth\"\n",
        "\n",
        "# # --- Gpu device --- #\n",
        "# device_ids = [Id for Id in range(torch.cuda.device_count())]\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # --- Define the network --- #\n",
        "# MyEnsembleNet = Custom_fusion_net().float()\n",
        "# print('MyEnsembleNet parameters:', sum(param.numel() for param in MyEnsembleNet.parameters()))\n",
        "\n",
        "# # --- Load testing data --- #\n",
        "# val_data = CustomDataLoader(HAZY_path = VAL_HAZY_IMAGES_PATH,\n",
        "#                             GT_path = VAL_GT_IMAGES_PATH,\n",
        "#                             resize = RESIZE)\n",
        "\n",
        "# val_loader = DataLoader(val_data, \n",
        "#                         batch_size = VAL_BATCH_SIZE, \n",
        "#                         num_workers = NUM_WORKERS)\n",
        "# MyEnsembleNet = MyEnsembleNet.to(device)\n",
        "\n",
        "# # --- Load the network weight --- #\n",
        "# try:\n",
        "#     MyEnsembleNet.load_state_dict(torch.load(G_model_save_dir))\n",
        "#     print('--- weight loaded ---')\n",
        "# except:\n",
        "#     print('--- no weight loaded ---')\n",
        "# # --- Start training --- #\n",
        "# print(\"-----Testing-----\")     \n",
        "# with torch.inference_mode():\n",
        "#     psnr_list = []\n",
        "#     ssim_list = []\n",
        "#     MyEnsembleNet.eval()\n",
        "#     for batch_idx, (hazy, clean, data_name) in enumerate(val_loader): \n",
        "#         clean = clean.to(device)\n",
        "#         hazy = hazy.to(device)\n",
        "#         frame_out, _ = MyEnsembleNet(hazy)\n",
        "#         psnr_list.extend(to_psnr(frame_out, clean))\n",
        "#         ssim_list.extend(to_ssim_skimage(frame_out, clean))\n",
        "#         if not os.path.exists('test/'):\n",
        "#             os.makedirs('test/')\n",
        "#         imwrite(frame_out, 'test/' + ''.join(data_name) + '.png', range=(0, 1))\n",
        "\n",
        "# avr_psnr = sum(psnr_list) / len(psnr_list)\n",
        "# avr_ssim = sum(ssim_list) / len(ssim_list)\n",
        "# print('PSNR: ', avr_psnr, 'SSIM: ', avr_ssim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srAETlUTGXaj"
      },
      "source": [
        "# Testing With Resizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7TB1ZZmGWHs",
        "outputId": "bc54ab1d-39f3-471c-a65c-5911db503291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyEnsembleNet parameters: 31417322\n",
            "--- weight loaded ---\n",
            "-----Testing-----\n",
            "PSNR:  20.746215161663343 SSIM:  0.753701651096344\n"
          ]
        }
      ],
      "source": [
        "# VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/valid_dense/HAZY/\"\n",
        "# VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/valid_dense/GT/\"\n",
        "\n",
        "VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_Hazy/\"\n",
        "VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_GT/\"\n",
        "\n",
        "# RESIZE = True\n",
        "\n",
        "#----------- Doesn't give Cuda out of memory -----------\n",
        "RESIZE = True \n",
        "\n",
        "TEST_IMAGE_SIZE = (768,1024) # won't be used in the data loader if RESIZE is set to false\n",
        "\n",
        "VAL_BATCH_SIZE = 1\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "\n",
        "# --- output picture and check point --- #\n",
        "G_model_save_dir = \"/content/drive/MyDrive/Graduation Project/CANT_Haze/Weights/Generator_NH_Restormer_Twice_HM_GBlock_AWB_Sobel_Best.pth\"\n",
        "# --- Gpu device --- #\n",
        "device_ids = [Id for Id in range(torch.cuda.device_count())]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Define the network --- #\n",
        "MyEnsembleNet = Custom_fusion_net().float()\n",
        "print('MyEnsembleNet parameters:', sum(param.numel() for param in MyEnsembleNet.parameters()))\n",
        "\n",
        "# --- Load testing data --- #\n",
        "val_data = CustomDataLoader(HAZY_path = VAL_HAZY_IMAGES_PATH,\n",
        "                            GT_path = VAL_GT_IMAGES_PATH,\n",
        "                            image_size = TEST_IMAGE_SIZE,\n",
        "                            resize = RESIZE)\n",
        "\n",
        "val_loader = DataLoader(val_data, \n",
        "                        batch_size = VAL_BATCH_SIZE, \n",
        "                        num_workers = NUM_WORKERS)\n",
        "MyEnsembleNet = MyEnsembleNet.to(device)\n",
        "\n",
        "# --- Load the network weight --- #\n",
        "try:\n",
        "    MyEnsembleNet.load_state_dict(torch.load(G_model_save_dir))\n",
        "    print('--- weight loaded ---')\n",
        "except:\n",
        "    print('--- no weight loaded ---')\n",
        "# --- Start training --- #\n",
        "print(\"-----Testing-----\")     \n",
        "with torch.inference_mode():\n",
        "    psnr_list = []\n",
        "    ssim_list = []\n",
        "    MyEnsembleNet.eval()\n",
        "    for batch_idx, (hazy, clean, data_name) in enumerate(val_loader): \n",
        "        clean = clean.to(device)\n",
        "        hazy = hazy.to(device)\n",
        "        frame_out, _ = MyEnsembleNet(hazy)\n",
        "        psnr_list.extend(to_psnr(frame_out, clean))\n",
        "        ssim_list.extend(to_ssim_skimage(frame_out, clean))\n",
        "        if not os.path.exists('test/'):\n",
        "            os.makedirs('test/')\n",
        "        imwrite(frame_out, 'test/' + ''.join(data_name) + '.png', range=(0, 1))\n",
        "\n",
        "avr_psnr = sum(psnr_list) / len(psnr_list)\n",
        "avr_ssim = sum(ssim_list) / len(ssim_list)\n",
        "print('PSNR: ', avr_psnr, 'SSIM: ', avr_ssim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQLL6WKkYwlu"
      },
      "source": [
        "# Testing With Cropping & Fusing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FrqO8dPYx7P"
      },
      "outputs": [],
      "source": [
        "# # VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/valid_dense/HAZY/\"\n",
        "# # VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/valid_dense/GT/\"\n",
        "\n",
        "# VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_Hazy/\"\n",
        "# VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_GT/\"\n",
        "\n",
        "# VAL_BATCH_SIZE = 1\n",
        "# NUM_WORKERS = 0\n",
        "\n",
        "# # --- output picture and check point --- #\n",
        "# G_model_save_dir = \"/content/drive/MyDrive/Graduation Project/CANT_Haze/Weights/Generator_NH_Restormer_Twice_HM_GBlock_AWB_Sobel_Best.pth\"\n",
        "# # --- Gpu device --- #\n",
        "# device_ids = [Id for Id in range(torch.cuda.device_count())]\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # --- Define the network --- #\n",
        "# MyEnsembleNet = Custom_fusion_net().float()\n",
        "# print('MyEnsembleNet parameters:', sum(param.numel() for param in MyEnsembleNet.parameters()))\n",
        "\n",
        "# # --- Load testing data --- #\n",
        "# val_data = dehaze_test_dataset(VAL_HAZY_IMAGES_PATH, VAL_GT_IMAGES_PATH)\n",
        "# val_loader = DataLoader(dataset=val_data, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "# MyEnsembleNet = MyEnsembleNet.to(device)\n",
        "\n",
        "# # --- Load the network weight --- #\n",
        "# try:\n",
        "#     MyEnsembleNet.load_state_dict(torch.load(G_model_save_dir))\n",
        "#     print('--- weight loaded ---')\n",
        "# except:\n",
        "#     print('--- no weight loaded ---')\n",
        "# # --- Start training --- #\n",
        "# print(\"-----Testing-----\")     \n",
        "# with torch.inference_mode():\n",
        "#     psnr_list = []\n",
        "#     ssim_list = []\n",
        "#     MyEnsembleNet.eval()\n",
        "#     for batch_idx, (hazy_up,hazy_down,name,clean) in enumerate(val_loader):\n",
        "#         hazy_up = hazy_up.to(device)\n",
        "#         hazy_down = hazy_down.to(device)\n",
        "#         clean = clean.to(device)\n",
        "#         frame_out_up, _ = MyEnsembleNet(hazy_up)\n",
        "#         frame_out_down, _ = MyEnsembleNet(hazy_down)\n",
        "#         #----------- With Cuda out of memory -----------\n",
        "#         frame_out = (torch.cat([frame_out_up[:, :, :600, :].permute(0, 2, 3, 1), frame_out_down[:, :, 552:, :].permute(0, 2, 3, 1)],1)).permute(0, 3, 1, 2)\n",
        "        \n",
        "#         #----------- Without Cuda out of memory -----------\n",
        "#         # frame_out = (torch.cat([frame_out_up[:, :, :600, :].permute(0, 2, 3, 1), frame_out_down[:, :, 168:, :].permute(0, 2, 3, 1)],1)).permute(0, 3, 1, 2)\n",
        "        \n",
        "#         psnr_list.extend(to_psnr(frame_out, clean))\n",
        "#         ssim_list.extend(to_ssim_skimage(frame_out, clean))\n",
        "#         if not os.path.exists('output/'):\n",
        "#             os.makedirs('output/')\n",
        "#         imwrite(frame_out, 'output/' + ''.join(name) + '.png', range=(0, 1))\n",
        "# avr_psnr = sum(psnr_list) / len(psnr_list)\n",
        "# avr_ssim = sum(ssim_list) / len(ssim_list)\n",
        "# print('PSNR: ', avr_psnr, 'SSIM: ', avr_ssim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3xpbP5AYKVO"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joXPuOBxYtrt"
      },
      "outputs": [],
      "source": [
        "# # --- train --- #\n",
        "# train_epoch = 100 # Currently at 1700 epochs\n",
        "# best_psnr = 20.75\n",
        "# # TRAIN_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/train_dense/haze/\"\n",
        "# # TRAIN_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/train_dense/GT/\"\n",
        "# # VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/valid_dense/HAZY/\"\n",
        "# # VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/valid_dense/GT/\"\n",
        "# TRAIN_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/train_NH/haze/\"\n",
        "# TRAIN_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/train_NH/clear_images/\"\n",
        "# VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_Hazy/\"\n",
        "# VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_GT/\"\n",
        "# IMAGE_SIZE = (256,256)\n",
        "# TRAIN_BATCH_SIZE = 1\n",
        "# VAL_BATCH_SIZE = 1\n",
        "# NUM_WORKERS = 0\n",
        "# SHUFFLE = True\n",
        "\n",
        "# # --- output picture and check point --- #\n",
        "# G_model_save_dir = \"/content/drive/MyDrive/Graduation Project/CANT_Haze/Weights/Generator_NH_Restormer_Twice_HM_GBlock_AWB_Sobel.pth\"\n",
        "# D_model_save_dir = \"/content/drive/MyDrive/Graduation Project/CANT_Haze/Weights/Discriminator_NH_Restormer_Twice_HM_GBlock_AWB_Sobel.pth\"\n",
        "# G_best_model_save_dir = \"/content/drive/MyDrive/Graduation Project/CANT_Haze/Weights/Generator_NH_Restormer_Twice_HM_GBlock_AWB_Sobel_Best.pth\"\n",
        "# D_best_model_save_dir = \"/content/drive/MyDrive/Graduation Project/CANT_Haze/Weights/Discriminator_NH_Restormer_Twice_HM_GBlock_AWB_Sobel_Best.pth\"\n",
        "\n",
        "# # --- Gpu device --- #\n",
        "# device_ids = [Id for Id in range(torch.cuda.device_count())]\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # --- Define the network --- #\n",
        "# MyEnsembleNet = Custom_fusion_net().float()\n",
        "# DNet = Discriminator()\n",
        "\n",
        "# print('MyEnsembleNet parameters:', sum(param.numel() for param in MyEnsembleNet.parameters()))\n",
        "\n",
        "# # --- Build optimizer --- #\n",
        "# G_optimizer = torch.optim.Adam(MyEnsembleNet.parameters(), lr=0.0001)\n",
        "# D_optim = torch.optim.Adam(DNet.parameters(), lr=0.0001)\n",
        "\n",
        "# # --- Load training data --- #\n",
        "# dataset = custom_dehaze_train_dataset(HAZY_path = TRAIN_HAZY_IMAGES_PATH, GT_path = TRAIN_GT_IMAGES_PATH, Image_Size = IMAGE_SIZE,is_train = True)\n",
        "# train_loader = DataLoader(dataset=dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "# # --- Load testing data --- #\n",
        "# val_data = dehaze_test_dataset(VAL_HAZY_IMAGES_PATH, VAL_GT_IMAGES_PATH)\n",
        "# val_loader = DataLoader(dataset=val_data, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "# MyEnsembleNet = MyEnsembleNet.to(device)\n",
        "# DNet = DNet.to(device)\n",
        "# # --- Load the network weight --- #\n",
        "# try:\n",
        "#     MyEnsembleNet.load_state_dict(torch.load(G_model_save_dir))\n",
        "#     DNet.load_state_dict(torch.load(D_model_save_dir))\n",
        "#     print('--- weight loaded ---')\n",
        "# except:\n",
        "#     print('--- no weight loaded ---')\n",
        "\n",
        "# # --- Define the perceptual loss network --- #\n",
        "# backbone_model = vgg16(weights = VGG16_Weights.DEFAULT)\n",
        "\n",
        "# backbone_model = backbone_model.features[:16].to(device)\n",
        "# for param in backbone_model.parameters():\n",
        "#      param.requires_grad = False\n",
        "\n",
        "# loss_network = LossNetwork(backbone_model)\n",
        "# loss_network.eval()\n",
        "# msssim_loss = msssim\n",
        "\n",
        "# # --- Start training --- #\n",
        "# for epoch in range(train_epoch):\n",
        "#     psnr_list = []\n",
        "#     ssim_list = []\n",
        "#     MyEnsembleNet.train()\n",
        "#     DNet.train()\n",
        "#     avg_loss = 0\n",
        "#     print(\"We are in epoch: \" + str(epoch+1))\n",
        "\n",
        "#     for batch_idx, (hazy, clean, clean_sobel) in enumerate(train_loader): \n",
        "#             hazy = hazy.to(device)\n",
        "#             clean = clean.to(device)\n",
        "#             clean_sobel = clean_sobel.to(device)\n",
        "#             output, hazy_sobel = MyEnsembleNet(hazy.float())\n",
        "#             real_out = DNet(clean)\n",
        "#             fake_out = DNet(output)\n",
        "#             real_loss = F.binary_cross_entropy(real_out, torch.ones(real_out.size()).to(device))\n",
        "#             fake_loss = F.binary_cross_entropy(fake_out, torch.zeros(fake_out.size()).to(device))\n",
        "#             D_loss = (real_loss + fake_loss) / 2\n",
        "#             DNet.zero_grad()\n",
        "#             D_loss.backward(retain_graph=True)\n",
        "#             smooth_loss_l1 = F.smooth_l1_loss(output, clean)\n",
        "#             perceptual_loss = loss_network(output, clean)\n",
        "#             msssim_loss_ = 1 - msssim_loss(output, clean, normalize=True)\n",
        "#             calc_psnr = to_psnr(output, clean)\n",
        "#             calc_ssim = to_ssim_skimage(output, clean)\n",
        "#             sobel_l1_loss = F.smooth_l1_loss(hazy_sobel, clean_sobel.float())\n",
        "#             sobel_msssim_loss = 1 - msssim_loss(hazy_sobel, clean_sobel.float(), normalize=True)\n",
        "#             total_loss = (smooth_loss_l1 + sobel_l1_loss)/2 + 0.05 * perceptual_loss +  (msssim_loss_ + sobel_msssim_loss)/2\n",
        "#             avg_loss += total_loss.item()\n",
        "#             MyEnsembleNet.zero_grad()\n",
        "#             total_loss.backward()\n",
        "#             G_optimizer.step()\n",
        "#             D_optim.step()\n",
        "#             psnr_list.extend(calc_psnr)\n",
        "#             ssim_list.extend(calc_ssim)\n",
        "    \n",
        "#     avr_psnr = sum(psnr_list) / len(psnr_list)\n",
        "#     avr_ssim = sum(ssim_list) / len(ssim_list)\n",
        "#     print('AVG PSNR: ', avr_psnr, 'AVG SSIM: ', avr_ssim, 'AVG Loss: ', avg_loss/len(psnr_list))\n",
        "\n",
        "#     if (epoch+1) % 5 == 0: \n",
        "#       print(\"-----Testing-----\")     \n",
        "#       with torch.inference_mode():\n",
        "#           psnr_list = []\n",
        "#           ssim_list = []\n",
        "#           MyEnsembleNet.eval()\n",
        "#           for batch_idx, (hazy_up,hazy_down,name,clean) in enumerate(val_loader):\n",
        "#               hazy_up = hazy_up.to(device)\n",
        "#               hazy_down = hazy_down.to(device)\n",
        "#               clean = clean.to(device)\n",
        "#               frame_out_up = MyEnsembleNet(hazy_up)\n",
        "#               frame_out_down = MyEnsembleNet(hazy_down)\n",
        "#               frame_out = (torch.cat([frame_out_up[:, :, 0:600, :].permute(0, 2, 3, 1), frame_out_down[:, :, 552:, :].permute(0, 2, 3, 1)],1)).permute(0, 3, 1, 2)\n",
        "#               psnr_list.extend(to_psnr(frame_out, clean))\n",
        "#               ssim_list.extend(to_ssim_skimage(frame_out, clean))\n",
        "#               imwrite(frame_out, '/content/drive/MyDrive/Graduation Project/CANT_Haze/Restormer Twice & AWB Results/' + ''.join(name) + '.png', range=(0, 1))\n",
        "\n",
        "#       avr_psnr = sum(psnr_list) / len(psnr_list)\n",
        "#       avr_ssim = sum(ssim_list) / len(ssim_list)\n",
        "#       print('PSNR: ', avr_psnr, 'SSIM: ', avr_ssim)    \n",
        "#       torch.save(MyEnsembleNet.state_dict(), G_model_save_dir)\n",
        "#       torch.save(DNet.state_dict(), D_model_save_dir)\n",
        "#       print(\"-----Model Saved-----\")\n",
        "#       if(avr_psnr > best_psnr):\n",
        "#           best_psnr = avr_psnr\n",
        "#           torch.save(MyEnsembleNet.state_dict(), G_best_model_save_dir)\n",
        "#           torch.save(DNet.state_dict(), D_best_model_save_dir)\n",
        "#           print(\"-----Best Model Saved-----\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "6037effae95b299fd169804fb49215cb60a78142f36519c409545155e1704ea9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
